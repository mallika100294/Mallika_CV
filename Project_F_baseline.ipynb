{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project F baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjBWjPit6//wK6OEKZeWMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallika100294/Mallika_CV/blob/master/Project_F_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_OvaN27oz0a",
        "colab_type": "code",
        "outputId": "077e9adc-a852-4829-d750-9618b2527201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWi4qFJyquQj",
        "colab_type": "code",
        "outputId": "de90dc26-aba2-482f-c173-d02e39ff8bd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Upgrade TensorFlow and Keras\n",
        "# !pip install --upgrade tensorflow\n",
        "# !pip install --upgrade keras\n",
        "\n",
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Keras imports\n",
        "from keras.preprocessing.text import one_hot, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Input\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model, Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, GlobalMaxPooling1D\n",
        "from keras.initializers import Constant\n",
        "from keras import backend as K\n",
        "import keras\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "BASE_DIR = 'drive/My Drive/ECE542-ProjectF/'\n",
        "DATA_DIR = BASE_DIR + 'data/'\n",
        "GLOVE_DIR = 'drive/My Drive/ECE542-ProjectF/data/GloVe'\n",
        "OUT_DIR = 'drive/My Drive/ECE542-ProjectF/output'\n",
        "# TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "ISEAR_SHUFFLE_SEED = 62\n",
        "KAGGLE_SHUFFLE_SEED = 43\n",
        "\n",
        "ISEAR_TEST_SPLIT = 0.1\n",
        "ISEAR_VALIDATION_SPLIT = 0.1\n",
        "KAGGLE_VALIDATION_SPLIT = 0.2\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OENtPorHyKzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(data, labels, val_split, test_split, seed):\n",
        "  # Shuffle our dataset. Using a known seed allows us to get the same result \n",
        "  # each time we run the script.\n",
        "  indices = np.arange(data.shape[0])\n",
        "  random.Random(seed).shuffle(indices)\n",
        "\n",
        "  data = data[indices]\n",
        "  labels = labels[indices]\n",
        "\n",
        "  # Determine number of samples in train/val/test\n",
        "  num_val = math.floor(data.shape[0] * val_split)\n",
        "  num_test = math.floor(data.shape[0] * test_split)\n",
        "\n",
        "  # Split the data into the test, val, and train subsets.\n",
        "  x_test = data[:num_test]\n",
        "  y_test = labels[:num_test]\n",
        "\n",
        "  x_val = data[num_test:num_val + num_test]\n",
        "  y_val = labels[num_test:num_val + num_test]\n",
        "\n",
        "  x_train = data[num_val + num_test:]\n",
        "  y_train = labels[num_val + num_test:]\n",
        "\n",
        "  # Return a dictionary of the data with train/val/test and x/y.\n",
        "  return {\n",
        "      'train': {\n",
        "          'x': x_train, 'y': y_train\n",
        "      },\n",
        "      'val': {\n",
        "          'x': x_val, 'y': y_val\n",
        "      },\n",
        "      'test': {\n",
        "          'x': x_test, 'y': y_test\n",
        "      }\n",
        "  }\n",
        "\n",
        "def prec_m(y_true, y_pred):\n",
        "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    pp = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = tp / (pp + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    pp = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = tp / (pp + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = prec_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision + recall + K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGKpmDUryT7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the ISEAR and Kaggle datasets and Tokenize the datasets\n",
        "# Both datasets have the label as their first index and the text as their second\n",
        "\n",
        "isear_text_data = []\n",
        "isear_labels = []\n",
        "kaggle_text_data = []\n",
        "kaggle_labels = []\n",
        "\n",
        "with open(('drive/My Drive/ECE542-ProjectF/data/isear/isear_clean.csv'), 'r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    isear_text_data.append(row[1])\n",
        "    isear_labels.append(int(row[0]))\n",
        "\n",
        "with open(('drive/My Drive/ECE542-ProjectF/data/kaggle/kaggle_clean.csv'), 'r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    kaggle_text_data.append(row[1])\n",
        "    kaggle_labels.append(int(row[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXZHf0QG4r2_",
        "colab_type": "code",
        "outputId": "040352b3-d2ad-405d-f233-2dc7b40ba7e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "isear_text_data_DF = pd.DataFrame(isear_text_data)\n",
        "isear_text_data_DF.columns =[\"text\"]\n",
        "isear_text_data_DF\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>During the period of falling in love, each tim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When I was involved in a traffic accident.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When I was driving home after  several days of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When I lost the person who meant the most to me.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The time I knocked a deer down - the sight of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7462</th>\n",
              "      <td>Two years back someone invited me to be the tu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7463</th>\n",
              "      <td>I had taken the responsibility to do something...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7464</th>\n",
              "      <td>I was at home and I heard a loud sound of spit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7465</th>\n",
              "      <td>I did not do the homework that the teacher had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7466</th>\n",
              "      <td>I had shouted at my younger brother and he was...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7467 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text\n",
              "0     During the period of falling in love, each tim...\n",
              "1            When I was involved in a traffic accident.\n",
              "2     When I was driving home after  several days of...\n",
              "3      When I lost the person who meant the most to me.\n",
              "4     The time I knocked a deer down - the sight of ...\n",
              "...                                                 ...\n",
              "7462  Two years back someone invited me to be the tu...\n",
              "7463  I had taken the responsibility to do something...\n",
              "7464  I was at home and I heard a loud sound of spit...\n",
              "7465  I did not do the homework that the teacher had...\n",
              "7466  I had shouted at my younger brother and he was...\n",
              "\n",
              "[7467 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYDXqV17Gt8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "isear_text_data_DF[\"text\"]\n",
        "isear_text_data_DF[\"text\"] = [entry.lower() for entry in isear_text_data_DF[\"text\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nsJipghCdoa",
        "colab_type": "code",
        "outputId": "cc9bd47a-3272-402d-9331-c693b0a9696b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "#nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "isear_text_data_DF['tweet_without_stopwords'] = isear_text_data_DF['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "isear_text_data_DF[['text','tweet_without_stopwords']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_without_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>during the period of falling in love, each tim...</td>\n",
              "      <td>period falling love, time met especially met l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>when i was involved in a traffic accident.</td>\n",
              "      <td>involved traffic accident.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>when i was driving home after  several days of...</td>\n",
              "      <td>driving home several days hard work, motorist ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>when i lost the person who meant the most to me.</td>\n",
              "      <td>lost person meant me.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the time i knocked a deer down - the sight of ...</td>\n",
              "      <td>time knocked deer - sight animal's injuries he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7462</th>\n",
              "      <td>two years back someone invited me to be the tu...</td>\n",
              "      <td>two years back someone invited tutor grand-dau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7463</th>\n",
              "      <td>i had taken the responsibility to do something...</td>\n",
              "      <td>taken responsibility something prepared it. ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7464</th>\n",
              "      <td>i was at home and i heard a loud sound of spit...</td>\n",
              "      <td>home heard loud sound spitting outside door. t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7465</th>\n",
              "      <td>i did not do the homework that the teacher had...</td>\n",
              "      <td>homework teacher asked us do. scolded immediat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7466</th>\n",
              "      <td>i had shouted at my younger brother and he was...</td>\n",
              "      <td>shouted younger brother always afraid called l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7467 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text                            tweet_without_stopwords\n",
              "0     during the period of falling in love, each tim...  period falling love, time met especially met l...\n",
              "1            when i was involved in a traffic accident.                         involved traffic accident.\n",
              "2     when i was driving home after  several days of...  driving home several days hard work, motorist ...\n",
              "3      when i lost the person who meant the most to me.                              lost person meant me.\n",
              "4     the time i knocked a deer down - the sight of ...  time knocked deer - sight animal's injuries he...\n",
              "...                                                 ...                                                ...\n",
              "7462  two years back someone invited me to be the tu...  two years back someone invited tutor grand-dau...\n",
              "7463  i had taken the responsibility to do something...  taken responsibility something prepared it. ho...\n",
              "7464  i was at home and i heard a loud sound of spit...  home heard loud sound spitting outside door. t...\n",
              "7465  i did not do the homework that the teacher had...  homework teacher asked us do. scolded immediat...\n",
              "7466  i had shouted at my younger brother and he was...  shouted younger brother always afraid called l...\n",
              "\n",
              "[7467 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chR95UOa1gmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tokenize ISEAR\n",
        "\n",
        "# ISEAR\n",
        "isear_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "isear_tokenizer.fit_on_texts(isear_text_data)\n",
        "isear_seq = isear_tokenizer.texts_to_sequences(isear_text_data)\n",
        "isear_index = isear_tokenizer.word_index\n",
        "isear_data = pad_sequences(isear_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "isear_labels = to_categorical(isear_labels)\n",
        "print('ISEAR Data Shapes:')\n",
        "print('Index size: ', len(isear_index))\n",
        "print('Data tensor: ', isear_data.shape)\n",
        "print('Label tensor: ', isear_labels.shape)\n",
        "\n",
        "# Shuffle our dataset. Using a known seed allows us to get the same result \n",
        "# each time we run the script.\n",
        "isear_split = split_data(\n",
        "    isear_data, \n",
        "    isear_labels, \n",
        "    ISEAR_VALIDATION_SPLIT, \n",
        "    ISEAR_TEST_SPLIT, \n",
        "    ISEAR_SHUFFLE_SEED\n",
        "  )\n",
        "\n",
        "print('ISEAR Split shapes:')\n",
        "for data_type, items in isear_split.items():\n",
        "  print('{0:5s}:'.format(data_type.capitalize()))\n",
        "  print('x: ', items['x'].shape, 'y: ', items['y'].shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTlAO5vkNdT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIPuwREP_9Zm",
        "colab_type": "code",
        "outputId": "8c52b826-c0ec-4317-e796-10ebadfba81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data=pandas.read_csv('drive/My Drive/ECE542-ProjectF/data/isear/isear_clean.csv')\n",
        "data=pandas.DataFrame(data)\n",
        "data.columns=['label','text']\n",
        "\n",
        "x_train, x_val, y_train, y_val = model_selection.train_test_split(data['text'], data['label'])\n",
        "\n",
        "\n",
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "  # fit the training dataset on the classifier\n",
        "  classifier.fit(feature_vector_train, label)\n",
        "  # predict the labels on validation dataset\n",
        "  predictions = classifier.predict(feature_vector_valid)\n",
        "  if is_neural_net:\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "  return metrics.accuracy_score(predictions, y_val)\n",
        "\n",
        "\n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=10000)\n",
        "tfidf_vect_ngram.fit(data['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(x_train)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(x_val)\n",
        "\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_val = encoder.fit_transform(y_val)\n",
        "\n",
        "\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM, N-Gram Vectors:  0.5757900374933048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXHR8lbrMUg5",
        "colab_type": "code",
        "outputId": "53385ea2-6b26-45e7-fff3-cc715c1542f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data=pd.read_csv('drive/My Drive/ECE542-ProjectF/data/isear/isear_clean.csv')\n",
        "data_DF=pd.DataFrame(data)\n",
        "data_DF.columns=['label','text']\n",
        "data_DF[\"text\"] = [entry.lower() for entry in data_DF[\"text\"]]\n",
        "stop = stopwords.words('english')\n",
        "data_DF['tweet_without_stopwords'] = data_DF['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "data_DF[['label','tweet_without_stopwords']]\n",
        "\n",
        "x_train, x_val, y_train, y_val = model_selection.train_test_split(data_DF['tweet_without_stopwords'], data_DF['label'])\n",
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "  # fit the training dataset on the classifier\n",
        "  classifier.fit(feature_vector_train, label)\n",
        "  # predict the labels on validation dataset\n",
        "  predictions = classifier.predict(feature_vector_valid)\n",
        "\n",
        "  if is_neural_net:\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "  return metrics.accuracy_score(predictions, y_val)\n",
        "\n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=10000)\n",
        "tfidf_vect_ngram.fit(data_DF['tweet_without_stopwords'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(x_train)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(x_val)\n",
        "\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_val = encoder.fit_transform(y_val)\n",
        "\n",
        "\n",
        "accuracy = train_model(svm.SVC(kernel='poly', degree=3, C=1.0), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM, N-Gram Vectors:  0.46009641135511514\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}